# Session Protocols

## On Session Start

Display this once at the beginning:
```
Cost-aware mode active.
Model: [current model] | /save to log | /clear resets context
```

## Cost Efficiency

Track approximate message count. At ~50 messages, suggest `/clear` if context allows. At ~100+, recommend fresh session.

If on Opus doing routine tasks, suggest Sonnet (`cc`). Reserve Opus for complex architecture only.

After major task completions, offer a checkpoint summary:
- What was done
- Files changed
- Next steps

If user sends 3+ small sequential requests, offer to batch them.

## Session End Protocol

When user says "done", "quit", "ending", or similar:
1. Remind them: "Run `/save` to save this session before quitting."

## User Context

- GitHub: Dicoangelo
- Projects: OS-App, Agentic Kernel, Antigravity ecosystem
- Aliases: `cq` (haiku), `cc` (sonnet), `co` (opus), `cstats`
- Skills: `/cost`, `/save`, `/history`

## Anthropic Products

- **Claude Code**: CLI agent for coding tasks (this tool)
- **Cowork**: Agent for non-coding tasks (research preview, Claude Max, macOS app)
  - Built-in VM isolation, browser automation, claude.ai data connectors
  - Access: Sidebar in Claude desktop app â†’ [claude.com/download](https://claude.com/download)

## Autonomous Routing System

**Status:** âœ… Active | **Version:** 1.0.0 | **Docs:** `~/.claude/ROUTING_SYSTEM_README.md`

### How It Works

The CLI automatically routes queries to the optimal model (Haiku, Sonnet, or Opus) using DQ scoring:
- **Haiku (C: 0.0-0.3):** Simple queries, quick answers, cheap
- **Sonnet (C: 0.3-0.7):** Code generation, analysis, moderate complexity
- **Opus (C: 0.7-1.0):** Architecture, complex reasoning, research

**DQ Score = Validity (40%) + Specificity (30%) + Correctness (30%)**

### Quick Commands

```bash
claude -p "query"              # Auto-routes (let it decide)
routing-dash                   # Performance dashboard
routing-report 7               # Weekly metrics
ai-feedback-enable             # Learn from failures

# Manual override when needed
claude --model opus -p "force opus for complex task"
```

### Key Features

- **Auto-routing:** Transparent decisions shown as `[DQ:0.75 C:0.45] â†’ sonnet`
- **Self-improving:** Learns from feedback, A/B tests optimizations
- **Research-driven:** Weekly arXiv sync updates baselines
- **Auto-update:** Applies validated improvements after 30-day stability

### Performance Targets

| Metric | Target | Status |
|--------|--------|--------|
| Routing Accuracy | â‰¥75% | ðŸ”„ Collecting |
| Cost Reduction | â‰¥20% vs random | ðŸ”„ Collecting |
| Latency (p95) | <50ms | âœ… ~42ms |

Run `routing-test-suite.py all` to check current performance.

### Documentation

- **Quick Ref:** `~/.claude/ROUTING_QUICK_REFERENCE.md`
- **Full Guide:** `~/.claude/ROUTING_SYSTEM_README.md`
- **Research:** `~/researchgravity/ROUTING_RESEARCH_WORKFLOW.md`

## Learned Patterns

<!-- AUTO-GENERATED BY META-ANALYZER - DO NOT EDIT MANUALLY -->
<!-- Last Updated: 2026-01-17T20:40:00 -->

### Usage Patterns Observed
- Peak productivity hours: 15:00, 14:00, 20:00
- Dominant session type: architecture (35%)
- Average session length: 265 messages
- Total sessions: 104 | Messages: 27,521

### Optimized Behaviors
- For architecture: Use Opus for system design, pre-load research papers
- For debugging: Start with /debug, escalate to Opus if >3 iterations
- For research: Pre-load learnings via `prefetch --papers`
- Cache efficiency: 99.88% - maintain by reusing context within sessions

### Cost Efficiency Rules
- Batch threshold: 3 sequential requests -> suggest batching
- Model routing: Use DQ scoring (validity 40% + specificity 30% + correctness 30%)
- Context: 50 messages suggest /clear | 100+ recommend fresh session

### Proactive Context
- Pattern prediction enabled via `prefetch --pattern <type>`
- Auto-detect session type from initial queries
- Pre-load relevant memory based on detected pattern

<!-- END AUTO-GENERATED -->
