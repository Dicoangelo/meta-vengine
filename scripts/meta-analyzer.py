#!/usr/bin/env python3
"""
Meta-Analyzer: Bidirectional Co-Evolution System

The core engine that enables Claude to analyze its own usage patterns
and modify its own instructions for continuous improvement.

Based on research:
- [2505.02888] [2503.00735] LADDER - Recursive refinement
- [2511.16931] OmniScientist - Co-evolving ecosystems
- [2512.20845] MAR - Multi-agent reflexion
- [2507.14241] Promptomatix - Prompt optimization
- [2510.24797] [2601.03511] IntroLM - Introspection prompts
- [2512.12686] Memoria - Retain, recall, reflect
- [2501.12689] IC-Cache - Token economics

Usage:
  meta-analyzer analyze           # Run analysis, show insights
  meta-analyzer propose           # Generate modification proposals
  meta-analyzer apply <id>        # Apply with approval
  meta-analyzer rollback <id>     # Revert changes
  meta-analyzer dashboard         # View effectiveness over time
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import hashlib

# ═══════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════

CLAUDE_DIR = Path.home() / ".claude"
KERNEL_DIR = CLAUDE_DIR / "kernel"
DATA_DIR = CLAUDE_DIR / "data"
SCRIPTS_DIR = CLAUDE_DIR / "scripts"
HISTORY_DIR = CLAUDE_DIR / "claude-md-history"
AGENT_CORE_DIR = Path.home() / ".agent-core"

# Data sources
STATS_CACHE = CLAUDE_DIR / "stats-cache.json"
DQ_SCORES = KERNEL_DIR / "dq-scores.jsonl"
ACTIVITY_EVENTS = DATA_DIR / "activity-events.jsonl"
DETECTED_PATTERNS = KERNEL_DIR / "detected-patterns.json"
IDENTITY_JSON = KERNEL_DIR / "identity.json"
LEARNINGS_MD = AGENT_CORE_DIR / "memory" / "learnings.md"

# Outputs
COEVO_CONFIG = KERNEL_DIR / "coevo-config.json"
MODIFICATIONS_LOG = KERNEL_DIR / "modifications.jsonl"
EFFECTIVENESS_LOG = KERNEL_DIR / "effectiveness.jsonl"
CLAUDE_MD = CLAUDE_DIR / "CLAUDE.md"

# Markers for auto-generated sections
LEARNED_START = "<!-- AUTO-GENERATED BY META-ANALYZER - DO NOT EDIT MANUALLY -->"
LEARNED_END = "<!-- END AUTO-GENERATED -->"


# ═══════════════════════════════════════════════════════════════════════════
# CONFIGURATION MANAGEMENT
# ═══════════════════════════════════════════════════════════════════════════

DEFAULT_CONFIG = {
    "version": "1.0.0",
    "enabled": True,
    "autoApply": False,
    "minConfidence": 0.7,
    "maxModificationsPerDay": 3,
    "rollbackOnEfficiencyDrop": 0.5,
    "trackingWindow": {
        "analysis": 7,  # days
        "effectiveness": 14  # days
    },
    "weights": {
        "usagePatterns": 0.3,
        "dqScores": 0.25,
        "cacheEfficiency": 0.2,
        "sessionDiversity": 0.15,
        "feedbackCorrelation": 0.1
    },
    "thresholds": {
        "significantPattern": 5,  # minimum occurrences
        "effectivenessCheck": 10  # sessions before evaluation
    }
}


def load_config() -> Dict[str, Any]:
    """Load or create coevo configuration."""
    if COEVO_CONFIG.exists():
        try:
            return json.loads(COEVO_CONFIG.read_text())
        except (json.JSONDecodeError, IOError):
            pass

    # Create default config
    KERNEL_DIR.mkdir(parents=True, exist_ok=True)
    COEVO_CONFIG.write_text(json.dumps(DEFAULT_CONFIG, indent=2))
    return DEFAULT_CONFIG


def save_config(config: Dict[str, Any]):
    """Save configuration."""
    COEVO_CONFIG.write_text(json.dumps(config, indent=2))


# ═══════════════════════════════════════════════════════════════════════════
# TELEMETRY AGGREGATION
# ═══════════════════════════════════════════════════════════════════════════

def load_stats_cache() -> Dict[str, Any]:
    """Load stats-cache.json."""
    if STATS_CACHE.exists():
        try:
            return json.loads(STATS_CACHE.read_text())
        except:
            pass
    return {}


def load_dq_scores() -> List[Dict[str, Any]]:
    """Load DQ score history."""
    scores = []
    if DQ_SCORES.exists():
        for line in DQ_SCORES.read_text().strip().split('\n'):
            if line:
                try:
                    scores.append(json.loads(line))
                except:
                    pass
    return scores


def load_activity_events() -> List[Dict[str, Any]]:
    """Load activity events."""
    events = []
    if ACTIVITY_EVENTS.exists():
        for line in ACTIVITY_EVENTS.read_text().strip().split('\n'):
            if line:
                try:
                    events.append(json.loads(line))
                except:
                    pass
    return events


def load_detected_patterns() -> Dict[str, Any]:
    """Load detected patterns."""
    if DETECTED_PATTERNS.exists():
        try:
            return json.loads(DETECTED_PATTERNS.read_text())
        except:
            pass
    return {}


def load_identity() -> Dict[str, Any]:
    """Load identity.json."""
    if IDENTITY_JSON.exists():
        try:
            return json.loads(IDENTITY_JSON.read_text())
        except:
            pass
    return {}


def load_learnings() -> str:
    """Load learnings.md content."""
    if LEARNINGS_MD.exists():
        return LEARNINGS_MD.read_text()
    return ""


def aggregate_telemetry(days: int = 7) -> Dict[str, Any]:
    """Aggregate all telemetry data sources into unified view."""
    cutoff = datetime.now() - timedelta(days=days)
    cutoff_ts = cutoff.timestamp() * 1000  # JS timestamps

    stats = load_stats_cache()
    dq_scores = load_dq_scores()
    activity = load_activity_events()
    patterns = load_detected_patterns()
    identity = load_identity()

    # Filter by time window
    recent_dq = [s for s in dq_scores if s.get('ts', 0) > cutoff_ts]
    recent_activity = [a for a in activity if a.get('timestamp', 0) > cutoff_ts]

    # Compute derived metrics
    total_messages = stats.get('totalMessages', 0)
    total_sessions = stats.get('totalSessions', 0)

    # Cache efficiency from model usage
    model_usage = stats.get('modelUsage', {})
    total_cache_read = sum(m.get('cacheReadInputTokens', 0) for m in model_usage.values())
    total_cache_create = sum(m.get('cacheCreationInputTokens', 0) for m in model_usage.values())
    total_input = sum(m.get('inputTokens', 0) for m in model_usage.values())

    if total_cache_read + total_cache_create + total_input > 0:
        cache_efficiency = total_cache_read / (total_cache_read + total_cache_create + total_input)
    else:
        cache_efficiency = 0

    # DQ score statistics
    if recent_dq:
        dq_avg = sum(s.get('dqScore', 0) for s in recent_dq) / len(recent_dq)
        dq_trend = _calculate_trend([s.get('dqScore', 0) for s in recent_dq])
    else:
        dq_avg = 0
        dq_trend = 0

    # Pattern distribution
    pattern_counts = {}
    for event in recent_activity:
        query = event.get('query', '').lower()
        for pattern_id in ['debugging', 'research', 'refactoring', 'testing',
                          'architecture', 'performance', 'deployment', 'learning']:
            if pattern_id in query or _matches_pattern_signals(query, pattern_id):
                pattern_counts[pattern_id] = pattern_counts.get(pattern_id, 0) + 1

    # Hour distribution
    hour_counts = stats.get('hourCounts', {})
    peak_hours = sorted(hour_counts.items(), key=lambda x: x[1], reverse=True)[:3]

    return {
        "aggregatedAt": datetime.now().isoformat(),
        "windowDays": days,
        "summary": {
            "totalMessages": total_messages,
            "totalSessions": total_sessions,
            "cacheEfficiency": round(cache_efficiency * 100, 2),
            "dqScoreAvg": round(dq_avg, 3),
            "dqTrend": dq_trend
        },
        "patterns": {
            "detected": patterns.get('patterns', []),
            "distribution": pattern_counts,
            "dominant": max(pattern_counts, key=pattern_counts.get) if pattern_counts else None
        },
        "temporal": {
            "peakHours": [int(h) for h, _ in peak_hours],
            "hourDistribution": hour_counts
        },
        "identity": {
            "expertise": identity.get('expertise', {}),
            "preferences": identity.get('preferences', {}),
            "achievements": len(identity.get('experience', {}).get('achievements', []))
        },
        "rawCounts": {
            "dqScores": len(recent_dq),
            "activityEvents": len(recent_activity)
        }
    }


def _calculate_trend(values: List[float]) -> str:
    """Calculate trend direction from value series."""
    if len(values) < 3:
        return "stable"

    first_half = sum(values[:len(values)//2]) / (len(values)//2)
    second_half = sum(values[len(values)//2:]) / (len(values) - len(values)//2)

    diff = second_half - first_half
    if diff > 0.05:
        return "improving"
    elif diff < -0.05:
        return "declining"
    return "stable"


def _matches_pattern_signals(text: str, pattern_id: str) -> bool:
    """Check if text matches pattern signals."""
    signals = {
        'debugging': ['error', 'fix', 'bug', 'debug', 'broken'],
        'research': ['arxiv', 'paper', 'research', 'study'],
        'refactoring': ['refactor', 'clean', 'restructure'],
        'testing': ['test', 'spec', 'coverage'],
        'architecture': ['architecture', 'design', 'system'],
        'performance': ['performance', 'optimize', 'slow'],
        'deployment': ['deploy', 'release', 'production'],
        'learning': ['learn', 'understand', 'explain']
    }

    for signal in signals.get(pattern_id, []):
        if signal in text:
            return True
    return False


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN ANALYSIS
# ═══════════════════════════════════════════════════════════════════════════

def analyze_patterns(telemetry: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze patterns and generate insights."""
    insights = []
    recommendations = []

    summary = telemetry.get('summary', {})
    patterns = telemetry.get('patterns', {})
    temporal = telemetry.get('temporal', {})
    identity = telemetry.get('identity', {})

    # Cache efficiency insight
    cache_eff = summary.get('cacheEfficiency', 0)
    if cache_eff > 99.5:
        insights.append({
            "type": "positive",
            "metric": "cacheEfficiency",
            "value": cache_eff,
            "message": f"Excellent cache efficiency at {cache_eff}%. Context reuse is optimal."
        })
    elif cache_eff < 95:
        insights.append({
            "type": "warning",
            "metric": "cacheEfficiency",
            "value": cache_eff,
            "message": f"Cache efficiency at {cache_eff}%. Consider longer sessions or context batching."
        })
        recommendations.append({
            "type": "behavior",
            "action": "Batch related queries into fewer sessions",
            "impact": "Could improve cache efficiency by 2-5%"
        })

    # DQ score insight
    dq_avg = summary.get('dqScoreAvg', 0)
    dq_trend = summary.get('dqTrend', 'stable')
    if dq_avg > 0.85:
        insights.append({
            "type": "positive",
            "metric": "dqScore",
            "value": dq_avg,
            "message": f"High decision quality ({dq_avg}). Model routing is well-calibrated."
        })
    elif dq_avg < 0.7:
        recommendations.append({
            "type": "calibration",
            "action": "Recalibrate DQ weights based on feedback correlation",
            "impact": "Could improve routing accuracy by 10-15%"
        })

    if dq_trend == "declining":
        recommendations.append({
            "type": "attention",
            "action": "Review recent routing decisions for mismatches",
            "impact": "Prevent further DQ score decline"
        })

    # Pattern distribution insight
    distribution = patterns.get('distribution', {})
    if distribution:
        dominant = patterns.get('dominant')
        dominant_pct = distribution.get(dominant, 0) / sum(distribution.values()) * 100 if distribution else 0

        insights.append({
            "type": "pattern",
            "metric": "sessionType",
            "value": dominant,
            "message": f"Dominant pattern: {dominant} ({dominant_pct:.0f}% of activity)"
        })

        if dominant_pct > 50:
            recommendations.append({
                "type": "context",
                "action": f"Pre-load {dominant}-specific context via prefetch --pattern {dominant}",
                "impact": "Faster session starts, more relevant suggestions"
            })

    # Temporal insight
    peak_hours = temporal.get('peakHours', [])
    if peak_hours:
        peak_str = ', '.join(f"{h}:00" for h in peak_hours[:3])
        insights.append({
            "type": "temporal",
            "metric": "peakHours",
            "value": peak_hours,
            "message": f"Peak productivity hours: {peak_str}"
        })

    # Session count insight
    total_sessions = summary.get('totalSessions', 0)
    total_messages = summary.get('totalMessages', 0)
    if total_sessions > 0:
        avg_messages = total_messages / total_sessions
        if avg_messages > 100:
            recommendations.append({
                "type": "efficiency",
                "action": "Consider /clear more often - avg session is {:.0f} messages".format(avg_messages),
                "impact": "Maintain context quality and cache efficiency"
            })

    return {
        "analyzedAt": datetime.now().isoformat(),
        "insights": insights,
        "recommendations": recommendations,
        "confidence": _calculate_confidence(telemetry)
    }


def _calculate_confidence(telemetry: Dict[str, Any]) -> float:
    """Calculate confidence in analysis based on data volume."""
    raw = telemetry.get('rawCounts', {})
    dq_count = raw.get('dqScores', 0)
    activity_count = raw.get('activityEvents', 0)

    # More data = higher confidence (caps at 0.95)
    data_score = min(0.95, (dq_count + activity_count) / 200)

    # Sessions count bonus
    sessions = telemetry.get('summary', {}).get('totalSessions', 0)
    session_score = min(0.3, sessions / 100)

    return round(min(0.95, data_score + session_score), 2)


# ═══════════════════════════════════════════════════════════════════════════
# MODIFICATION GENERATION
# ═══════════════════════════════════════════════════════════════════════════

def generate_modifications(analysis: Dict[str, Any], telemetry: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate modification proposals from analysis."""
    modifications = []
    mod_id_base = datetime.now().strftime("%Y%m%d")

    recommendations = analysis.get('recommendations', [])
    insights = analysis.get('insights', [])
    confidence = analysis.get('confidence', 0)

    for i, rec in enumerate(recommendations):
        mod_id = f"mod-{mod_id_base}-{i:03d}"

        mod = {
            "id": mod_id,
            "createdAt": datetime.now().isoformat(),
            "type": rec.get('type'),
            "action": rec.get('action'),
            "impact": rec.get('impact'),
            "confidence": confidence,
            "status": "proposed",
            "target": _determine_target(rec),
            "changes": _generate_changes(rec, telemetry)
        }

        modifications.append(mod)

    # Generate CLAUDE.md update modification
    claude_md_mod = _generate_claude_md_update(insights, telemetry)
    if claude_md_mod:
        modifications.append(claude_md_mod)

    return modifications


def _determine_target(recommendation: Dict[str, Any]) -> str:
    """Determine which file/component the modification targets."""
    rec_type = recommendation.get('type', '')

    if rec_type in ['behavior', 'attention']:
        return 'CLAUDE.md'
    elif rec_type == 'calibration':
        return 'dq-scorer.js'
    elif rec_type == 'context':
        return 'prefetch.py'
    elif rec_type == 'efficiency':
        return 'CLAUDE.md'

    return 'CLAUDE.md'


def _generate_changes(recommendation: Dict[str, Any], telemetry: Dict[str, Any]) -> Dict[str, Any]:
    """Generate specific changes for a recommendation."""
    rec_type = recommendation.get('type', '')

    if rec_type == 'calibration':
        # DQ weight recalibration
        return {
            "file": "~/.claude/kernel/dq-scorer.js",
            "type": "weight_adjustment",
            "current": {"validity": 0.4, "specificity": 0.3, "correctness": 0.3},
            "proposed": {"validity": 0.35, "specificity": 0.35, "correctness": 0.3}
        }

    if rec_type == 'context':
        pattern = telemetry.get('patterns', {}).get('dominant', 'general')
        return {
            "file": "~/.claude/CLAUDE.md",
            "type": "add_instruction",
            "section": "Learned Patterns",
            "content": f"Default prefetch pattern: {pattern}"
        }

    return {"type": "note", "content": recommendation.get('action', '')}


def _generate_claude_md_update(insights: List[Dict], telemetry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Generate CLAUDE.md learned patterns section update."""
    if not insights:
        return None

    summary = telemetry.get('summary', {})
    patterns = telemetry.get('patterns', {})
    temporal = telemetry.get('temporal', {})

    # Build the auto-generated section
    lines = [
        "",
        "## Learned Patterns",
        "",
        LEARNED_START,
        f"<!-- Last Updated: {datetime.now().isoformat()} -->",
        "",
        "### Usage Patterns Observed",
    ]

    # Peak hours
    peak_hours = temporal.get('peakHours', [])
    if peak_hours:
        peak_str = ', '.join(f"{h}:00" for h in peak_hours[:3])
        sessions = summary.get('totalSessions', 0)
        lines.append(f"- Peak productivity hours: {peak_str}")

    # Dominant pattern
    dominant = patterns.get('dominant')
    distribution = patterns.get('distribution', {})
    if dominant and distribution:
        pct = distribution.get(dominant, 0) / sum(distribution.values()) * 100
        lines.append(f"- Dominant session type: {dominant} ({pct:.0f}%)")

    # Session stats
    avg_duration = summary.get('totalMessages', 0) / max(1, summary.get('totalSessions', 1))
    lines.append(f"- Average session length: {avg_duration:.0f} messages")

    lines.append("")
    lines.append("### Optimized Behaviors")

    # Add behavioral recommendations
    if dominant == 'debugging':
        lines.append("- For debugging: Start with /debug, escalate to Opus if >3 iterations")
    elif dominant == 'research':
        lines.append("- For research: Pre-load learnings via prefetch --papers")
    elif dominant == 'refactoring':
        lines.append("- For refactoring: Run tests before AND after changes")
    else:
        lines.append("- Use pattern-aware prefetch for faster context loading")

    # Cache efficiency note
    cache_eff = summary.get('cacheEfficiency', 0)
    lines.append(f"- Cache efficiency: {cache_eff}% - maintain by reusing context")

    # Batch threshold
    lines.append("- Batch threshold: 3 sequential requests -> suggest batching")

    lines.append("")
    lines.append(LEARNED_END)
    lines.append("")

    return {
        "id": f"mod-{datetime.now().strftime('%Y%m%d')}-claude-md",
        "createdAt": datetime.now().isoformat(),
        "type": "claude_md_update",
        "action": "Update CLAUDE.md with learned patterns",
        "impact": "Better session initialization and context awareness",
        "confidence": 0.9,
        "status": "proposed",
        "target": "CLAUDE.md",
        "changes": {
            "file": str(CLAUDE_MD),
            "type": "section_update",
            "section": "Learned Patterns",
            "content": '\n'.join(lines)
        }
    }


# ═══════════════════════════════════════════════════════════════════════════
# MODIFICATION APPLICATION
# ═══════════════════════════════════════════════════════════════════════════

def apply_modification(mod_id: str, dry_run: bool = False) -> Dict[str, Any]:
    """Apply a modification with rollback support."""

    # Load modification
    mod = _find_modification(mod_id)
    if not mod:
        return {"success": False, "error": f"Modification {mod_id} not found"}

    if mod.get('status') == 'applied':
        return {"success": False, "error": f"Modification {mod_id} already applied"}

    changes = mod.get('changes', {})
    target_file = changes.get('file', '')
    change_type = changes.get('type', '')

    result = {"mod_id": mod_id, "dry_run": dry_run}

    if change_type == 'section_update' and 'CLAUDE.md' in target_file:
        # Handle CLAUDE.md section update
        backup_path = _backup_claude_md()
        result['backup'] = str(backup_path)

        if not dry_run:
            success = _update_claude_md_section(changes.get('content', ''))
            if success:
                _log_modification(mod_id, 'applied', backup_path)
                result['success'] = True
                result['message'] = "CLAUDE.md updated with learned patterns"
            else:
                result['success'] = False
                result['error'] = "Failed to update CLAUDE.md"
        else:
            result['success'] = True
            result['message'] = f"Would update CLAUDE.md (backup at {backup_path})"
            result['preview'] = changes.get('content', '')[:500]

    else:
        result['success'] = False
        result['error'] = f"Unknown change type: {change_type}"

    return result


def _find_modification(mod_id: str) -> Optional[Dict[str, Any]]:
    """Find a modification by ID."""
    if not MODIFICATIONS_LOG.exists():
        return None

    for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
        if line:
            try:
                mod = json.loads(line)
                if mod.get('id') == mod_id:
                    return mod
            except:
                pass

    return None


def _backup_claude_md() -> Path:
    """Backup CLAUDE.md before modification."""
    HISTORY_DIR.mkdir(parents=True, exist_ok=True)

    if CLAUDE_MD.exists():
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = HISTORY_DIR / f"CLAUDE.md.{timestamp}"
        backup_path.write_text(CLAUDE_MD.read_text())

        # Initialize git if not exists
        git_dir = HISTORY_DIR / ".git"
        if not git_dir.exists():
            subprocess.run(['git', 'init'], cwd=HISTORY_DIR, capture_output=True)

        # Commit the backup
        subprocess.run(['git', 'add', '.'], cwd=HISTORY_DIR, capture_output=True)
        subprocess.run(['git', 'commit', '-m', f'Backup before {timestamp}'],
                      cwd=HISTORY_DIR, capture_output=True)

        return backup_path

    return HISTORY_DIR / "CLAUDE.md.empty"


def _update_claude_md_section(new_content: str) -> bool:
    """Update the learned patterns section in CLAUDE.md."""
    if not CLAUDE_MD.exists():
        return False

    content = CLAUDE_MD.read_text()

    # Check if section exists
    if LEARNED_START in content and LEARNED_END in content:
        # Replace existing section
        pattern = rf"## Learned Patterns\s*\n{re.escape(LEARNED_START)}.*?{re.escape(LEARNED_END)}"
        new_content_clean = new_content.strip()
        content = re.sub(pattern, new_content_clean, content, flags=re.DOTALL)
    else:
        # Add new section at end
        content = content.rstrip() + "\n\n" + new_content.strip() + "\n"

    CLAUDE_MD.write_text(content)
    return True


def _log_modification(mod_id: str, status: str, backup_path: Optional[Path] = None):
    """Log modification application."""
    entry = {
        "mod_id": mod_id,
        "status": status,
        "appliedAt": datetime.now().isoformat(),
        "backup": str(backup_path) if backup_path else None
    }

    with open(MODIFICATIONS_LOG, 'a') as f:
        f.write(json.dumps(entry) + '\n')


def rollback_modification(mod_id: str) -> Dict[str, Any]:
    """Rollback a modification."""
    # Find the modification and its backup
    if not MODIFICATIONS_LOG.exists():
        return {"success": False, "error": "No modifications log found"}

    for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
        if line:
            try:
                entry = json.loads(line)
                if entry.get('mod_id') == mod_id and entry.get('backup'):
                    backup_path = Path(entry['backup'])
                    if backup_path.exists():
                        CLAUDE_MD.write_text(backup_path.read_text())
                        _log_modification(mod_id, 'rolled_back')
                        return {
                            "success": True,
                            "message": f"Rolled back {mod_id} from {backup_path}"
                        }
            except:
                pass

    return {"success": False, "error": f"No backup found for {mod_id}"}


# ═══════════════════════════════════════════════════════════════════════════
# EFFECTIVENESS TRACKING
# ═══════════════════════════════════════════════════════════════════════════

def evaluate_effectiveness(mod_id: str, sessions: int = 10) -> Dict[str, Any]:
    """Evaluate effectiveness of a modification."""
    # Load before/after metrics
    stats = load_stats_cache()
    dq_scores = load_dq_scores()

    # Find modification timestamp
    mod = _find_modification(mod_id)
    if not mod:
        return {"success": False, "error": f"Modification {mod_id} not found"}

    applied_at = mod.get('appliedAt')
    if not applied_at:
        return {"success": False, "error": "Modification not applied"}

    applied_ts = datetime.fromisoformat(applied_at).timestamp() * 1000

    # Split scores into before/after
    before = [s for s in dq_scores if s.get('ts', 0) < applied_ts]
    after = [s for s in dq_scores if s.get('ts', 0) >= applied_ts]

    if len(after) < sessions:
        return {
            "success": False,
            "error": f"Not enough sessions yet ({len(after)}/{sessions})"
        }

    # Calculate metrics
    before_dq = sum(s.get('dqScore', 0) for s in before[-sessions:]) / sessions if before else 0
    after_dq = sum(s.get('dqScore', 0) for s in after[:sessions]) / sessions

    improvement = after_dq - before_dq
    significant = abs(improvement) > 0.05

    result = {
        "mod_id": mod_id,
        "metric": "dq_score_avg",
        "before": round(before_dq, 3),
        "after": round(after_dq, 3),
        "improvement": round(improvement, 3),
        "sessionsCompared": sessions,
        "statisticallySignificant": significant,
        "evaluatedAt": datetime.now().isoformat()
    }

    # Log effectiveness
    with open(EFFECTIVENESS_LOG, 'a') as f:
        f.write(json.dumps(result) + '\n')

    return result


def get_dashboard() -> Dict[str, Any]:
    """Get dashboard view of co-evolution system."""
    config = load_config()
    telemetry = aggregate_telemetry(days=config['trackingWindow']['analysis'])

    # Load modification history
    modifications = []
    if MODIFICATIONS_LOG.exists():
        for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
            if line:
                try:
                    modifications.append(json.loads(line))
                except:
                    pass

    # Load effectiveness history
    effectiveness = []
    if EFFECTIVENESS_LOG.exists():
        for line in EFFECTIVENESS_LOG.read_text().strip().split('\n'):
            if line:
                try:
                    effectiveness.append(json.loads(line))
                except:
                    pass

    return {
        "generatedAt": datetime.now().isoformat(),
        "config": {
            "enabled": config.get('enabled', True),
            "autoApply": config.get('autoApply', False),
            "minConfidence": config.get('minConfidence', 0.7)
        },
        "summary": telemetry.get('summary', {}),
        "patterns": telemetry.get('patterns', {}),
        "modifications": {
            "total": len(modifications),
            "applied": len([m for m in modifications if m.get('status') == 'applied']),
            "rolledBack": len([m for m in modifications if m.get('status') == 'rolled_back']),
            "recent": modifications[-5:]
        },
        "effectiveness": {
            "evaluations": len(effectiveness),
            "avgImprovement": sum(e.get('improvement', 0) for e in effectiveness) / len(effectiveness) if effectiveness else 0,
            "recent": effectiveness[-5:]
        }
    }


# ═══════════════════════════════════════════════════════════════════════════
# CLI INTERFACE
# ═══════════════════════════════════════════════════════════════════════════

def main():
    parser = argparse.ArgumentParser(
        description="Meta-Analyzer: Bidirectional Co-Evolution System"
    )

    subparsers = parser.add_subparsers(dest='command', help='Commands')

    # analyze
    analyze_parser = subparsers.add_parser('analyze', help='Run analysis and show insights')
    analyze_parser.add_argument('--days', '-d', type=int, default=7, help='Days to analyze')
    analyze_parser.add_argument('--json', action='store_true', help='Output as JSON')

    # propose
    propose_parser = subparsers.add_parser('propose', help='Generate modification proposals')
    propose_parser.add_argument('--json', action='store_true', help='Output as JSON')

    # apply
    apply_parser = subparsers.add_parser('apply', help='Apply a modification')
    apply_parser.add_argument('mod_id', help='Modification ID')
    apply_parser.add_argument('--dry-run', action='store_true', help='Preview without applying')

    # rollback
    rollback_parser = subparsers.add_parser('rollback', help='Rollback a modification')
    rollback_parser.add_argument('mod_id', help='Modification ID')

    # evaluate
    eval_parser = subparsers.add_parser('evaluate', help='Evaluate modification effectiveness')
    eval_parser.add_argument('mod_id', help='Modification ID')
    eval_parser.add_argument('--sessions', type=int, default=10, help='Sessions to compare')

    # dashboard
    dash_parser = subparsers.add_parser('dashboard', help='View system dashboard')
    dash_parser.add_argument('--json', action='store_true', help='Output as JSON')

    # config
    config_parser = subparsers.add_parser('config', help='View/modify configuration')
    config_parser.add_argument('--set', nargs=2, metavar=('KEY', 'VALUE'), help='Set config value')

    args = parser.parse_args()

    if args.command == 'analyze':
        telemetry = aggregate_telemetry(days=args.days)
        analysis = analyze_patterns(telemetry)

        if args.json:
            print(json.dumps({"telemetry": telemetry, "analysis": analysis}, indent=2))
        else:
            print(f"\n{'='*60}")
            print("META-ANALYZER: Co-Evolution Analysis")
            print(f"{'='*60}\n")

            summary = telemetry.get('summary', {})
            print(f"Sessions: {summary.get('totalSessions', 0)} | Messages: {summary.get('totalMessages', 0)}")
            print(f"Cache Efficiency: {summary.get('cacheEfficiency', 0)}%")
            print(f"DQ Score Avg: {summary.get('dqScoreAvg', 0)} ({summary.get('dqTrend', 'stable')})")
            print(f"Confidence: {analysis.get('confidence', 0)}")

            print(f"\n--- Insights ---")
            for insight in analysis.get('insights', []):
                icon = "+" if insight['type'] == 'positive' else "!" if insight['type'] == 'warning' else "*"
                print(f"  [{icon}] {insight['message']}")

            print(f"\n--- Recommendations ---")
            for rec in analysis.get('recommendations', []):
                print(f"  -> {rec['action']}")
                print(f"     Impact: {rec['impact']}")

            print()

    elif args.command == 'propose':
        telemetry = aggregate_telemetry()
        analysis = analyze_patterns(telemetry)
        modifications = generate_modifications(analysis, telemetry)

        # Save proposals
        for mod in modifications:
            with open(MODIFICATIONS_LOG, 'a') as f:
                f.write(json.dumps(mod) + '\n')

        if args.json:
            print(json.dumps(modifications, indent=2))
        else:
            print(f"\nGenerated {len(modifications)} modification proposals:\n")
            for mod in modifications:
                print(f"  {mod['id']}: {mod['action']}")
                print(f"     Type: {mod['type']} | Target: {mod['target']}")
                print(f"     Confidence: {mod['confidence']} | Impact: {mod.get('impact', 'N/A')}")
                print()

            print("Apply with: meta-analyzer apply <mod_id>")
            print("Preview with: meta-analyzer apply <mod_id> --dry-run")

    elif args.command == 'apply':
        result = apply_modification(args.mod_id, dry_run=args.dry_run)
        print(json.dumps(result, indent=2))

    elif args.command == 'rollback':
        result = rollback_modification(args.mod_id)
        print(json.dumps(result, indent=2))

    elif args.command == 'evaluate':
        result = evaluate_effectiveness(args.mod_id, sessions=args.sessions)
        print(json.dumps(result, indent=2))

    elif args.command == 'dashboard':
        dashboard = get_dashboard()

        if args.json:
            print(json.dumps(dashboard, indent=2))
        else:
            print(f"\n{'='*60}")
            print("CO-EVOLUTION DASHBOARD")
            print(f"{'='*60}\n")

            config = dashboard.get('config', {})
            print(f"Status: {'ENABLED' if config.get('enabled') else 'DISABLED'}")
            print(f"Auto-Apply: {'ON' if config.get('autoApply') else 'OFF'}")
            print(f"Min Confidence: {config.get('minConfidence', 0.7)}")

            summary = dashboard.get('summary', {})
            print(f"\n--- Metrics ---")
            print(f"Cache Efficiency: {summary.get('cacheEfficiency', 0)}%")
            print(f"DQ Score Avg: {summary.get('dqScoreAvg', 0)}")

            mods = dashboard.get('modifications', {})
            print(f"\n--- Modifications ---")
            print(f"Total: {mods.get('total', 0)} | Applied: {mods.get('applied', 0)} | Rolled Back: {mods.get('rolledBack', 0)}")

            eff = dashboard.get('effectiveness', {})
            print(f"\n--- Effectiveness ---")
            print(f"Evaluations: {eff.get('evaluations', 0)}")
            print(f"Avg Improvement: {eff.get('avgImprovement', 0):.3f}")
            print()

    elif args.command == 'config':
        config = load_config()

        if args.set:
            key, value = args.set
            # Parse value
            if value.lower() == 'true':
                value = True
            elif value.lower() == 'false':
                value = False
            elif value.replace('.', '').isdigit():
                value = float(value) if '.' in value else int(value)

            config[key] = value
            save_config(config)
            print(f"Set {key} = {value}")
        else:
            print(json.dumps(config, indent=2))

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
