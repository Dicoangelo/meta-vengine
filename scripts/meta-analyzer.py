#!/usr/bin/env python3
"""
Meta-Analyzer: Bidirectional Co-Evolution System

The core engine that enables Claude to analyze its own usage patterns
and modify its own instructions for continuous improvement.

Based on research:
- [2505.02888] [2503.00735] LADDER - Recursive refinement
- [2511.16931] OmniScientist - Co-evolving ecosystems
- [2512.20845] MAR - Multi-agent reflexion
- [2507.14241] Promptomatix - Prompt optimization
- [2510.24797] [2601.03511] IntroLM - Introspection prompts
- [2512.12686] Memoria - Retain, recall, reflect
- [2501.12689] IC-Cache - Token economics

Usage:
  meta-analyzer analyze           # Run analysis, show insights
  meta-analyzer propose           # Generate modification proposals
  meta-analyzer apply <id>        # Apply with approval
  meta-analyzer rollback <id>     # Revert changes
  meta-analyzer dashboard         # View effectiveness over time
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import hashlib

# ═══════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════

CLAUDE_DIR = Path.home() / ".claude"
KERNEL_DIR = CLAUDE_DIR / "kernel"
DATA_DIR = CLAUDE_DIR / "data"
SCRIPTS_DIR = CLAUDE_DIR / "scripts"
HISTORY_DIR = CLAUDE_DIR / "claude-md-history"
AGENT_CORE_DIR = Path.home() / ".agent-core"

# Data sources
STATS_CACHE = CLAUDE_DIR / "stats-cache.json"
DQ_SCORES = KERNEL_DIR / "dq-scores.jsonl"
ACTIVITY_EVENTS = DATA_DIR / "activity-events.jsonl"
DETECTED_PATTERNS = KERNEL_DIR / "detected-patterns.json"
IDENTITY_JSON = KERNEL_DIR / "identity.json"
LEARNINGS_MD = AGENT_CORE_DIR / "memory" / "learnings.md"

# Outputs
COEVO_CONFIG = KERNEL_DIR / "coevo-config.json"
MODIFICATIONS_LOG = KERNEL_DIR / "modifications.jsonl"
EFFECTIVENESS_LOG = KERNEL_DIR / "effectiveness.jsonl"
CLAUDE_MD = CLAUDE_DIR / "CLAUDE.md"

# Markers for auto-generated sections
LEARNED_START = "<!-- AUTO-GENERATED BY META-ANALYZER - DO NOT EDIT MANUALLY -->"
LEARNED_END = "<!-- END AUTO-GENERATED -->"


# ═══════════════════════════════════════════════════════════════════════════
# CONFIGURATION MANAGEMENT
# ═══════════════════════════════════════════════════════════════════════════

DEFAULT_CONFIG = {
    "version": "1.0.0",
    "enabled": True,
    "autoApply": False,
    "minConfidence": 0.7,
    "maxModificationsPerDay": 3,
    "rollbackOnEfficiencyDrop": 0.5,
    "trackingWindow": {
        "analysis": 7,  # days
        "effectiveness": 14  # days
    },
    "weights": {
        "usagePatterns": 0.3,
        "dqScores": 0.25,
        "cacheEfficiency": 0.2,
        "sessionDiversity": 0.15,
        "feedbackCorrelation": 0.1
    },
    "thresholds": {
        "significantPattern": 5,  # minimum occurrences
        "effectivenessCheck": 10  # sessions before evaluation
    }
}


def load_config() -> Dict[str, Any]:
    """Load or create coevo configuration."""
    if COEVO_CONFIG.exists():
        try:
            return json.loads(COEVO_CONFIG.read_text())
        except (json.JSONDecodeError, IOError):
            pass

    # Create default config
    KERNEL_DIR.mkdir(parents=True, exist_ok=True)
    COEVO_CONFIG.write_text(json.dumps(DEFAULT_CONFIG, indent=2))
    return DEFAULT_CONFIG


def save_config(config: Dict[str, Any]):
    """Save configuration."""
    COEVO_CONFIG.write_text(json.dumps(config, indent=2))


# ═══════════════════════════════════════════════════════════════════════════
# TELEMETRY AGGREGATION
# ═══════════════════════════════════════════════════════════════════════════

def load_stats_cache() -> Dict[str, Any]:
    """Load stats-cache.json."""
    if STATS_CACHE.exists():
        try:
            return json.loads(STATS_CACHE.read_text())
        except:
            pass
    return {}


def load_dq_scores() -> List[Dict[str, Any]]:
    """Load DQ score history."""
    scores = []
    if DQ_SCORES.exists():
        for line in DQ_SCORES.read_text().strip().split('\n'):
            if line:
                try:
                    scores.append(json.loads(line))
                except:
                    pass
    return scores


def load_activity_events() -> List[Dict[str, Any]]:
    """Load activity events."""
    events = []
    if ACTIVITY_EVENTS.exists():
        for line in ACTIVITY_EVENTS.read_text().strip().split('\n'):
            if line:
                try:
                    events.append(json.loads(line))
                except:
                    pass
    return events


def load_detected_patterns() -> Dict[str, Any]:
    """Load detected patterns."""
    if DETECTED_PATTERNS.exists():
        try:
            return json.loads(DETECTED_PATTERNS.read_text())
        except:
            pass
    return {}


def load_identity() -> Dict[str, Any]:
    """Load identity.json."""
    if IDENTITY_JSON.exists():
        try:
            return json.loads(IDENTITY_JSON.read_text())
        except:
            pass
    return {}


def load_learnings() -> str:
    """Load learnings.md content."""
    if LEARNINGS_MD.exists():
        return LEARNINGS_MD.read_text()
    return ""


def aggregate_telemetry(days: int = 7) -> Dict[str, Any]:
    """Aggregate all telemetry data sources into unified view."""
    cutoff = datetime.now() - timedelta(days=days)
    cutoff_ts = cutoff.timestamp() * 1000  # JS timestamps

    stats = load_stats_cache()
    dq_scores = load_dq_scores()
    activity = load_activity_events()
    patterns = load_detected_patterns()
    identity = load_identity()

    # Filter by time window
    recent_dq = [s for s in dq_scores if s.get('ts', 0) > cutoff_ts]
    recent_activity = [a for a in activity if a.get('timestamp', 0) > cutoff_ts]

    # Compute derived metrics
    total_messages = stats.get('totalMessages', 0)
    total_sessions = stats.get('totalSessions', 0)

    # Cache efficiency from model usage
    model_usage = stats.get('modelUsage', {})
    total_cache_read = sum(m.get('cacheReadInputTokens', 0) for m in model_usage.values())
    total_cache_create = sum(m.get('cacheCreationInputTokens', 0) for m in model_usage.values())
    total_input = sum(m.get('inputTokens', 0) for m in model_usage.values())

    if total_cache_read + total_cache_create + total_input > 0:
        cache_efficiency = total_cache_read / (total_cache_read + total_cache_create + total_input)
    else:
        cache_efficiency = 0

    # DQ score statistics
    if recent_dq:
        dq_avg = sum(s.get('dqScore', 0) for s in recent_dq) / len(recent_dq)
        dq_trend = _calculate_trend([s.get('dqScore', 0) for s in recent_dq])
    else:
        dq_avg = 0
        dq_trend = 0

    # Pattern distribution
    pattern_counts = {}
    for event in recent_activity:
        query = event.get('query', '').lower()
        for pattern_id in ['debugging', 'research', 'refactoring', 'testing',
                          'architecture', 'performance', 'deployment', 'learning']:
            if pattern_id in query or _matches_pattern_signals(query, pattern_id):
                pattern_counts[pattern_id] = pattern_counts.get(pattern_id, 0) + 1

    # Hour distribution
    hour_counts = stats.get('hourCounts', {})
    peak_hours = sorted(hour_counts.items(), key=lambda x: x[1], reverse=True)[:3]

    return {
        "aggregatedAt": datetime.now().isoformat(),
        "windowDays": days,
        "summary": {
            "totalMessages": total_messages,
            "totalSessions": total_sessions,
            "cacheEfficiency": round(cache_efficiency * 100, 2),
            "dqScoreAvg": round(dq_avg, 3),
            "dqTrend": dq_trend
        },
        "patterns": {
            "detected": patterns.get('patterns', []),
            "distribution": pattern_counts,
            "dominant": max(pattern_counts, key=pattern_counts.get) if pattern_counts else None
        },
        "temporal": {
            "peakHours": [int(h) for h, _ in peak_hours],
            "hourDistribution": hour_counts
        },
        "identity": {
            "expertise": identity.get('expertise', {}),
            "preferences": identity.get('preferences', {}),
            "achievements": len(identity.get('experience', {}).get('achievements', []))
        },
        "rawCounts": {
            "dqScores": len(recent_dq),
            "activityEvents": len(recent_activity)
        }
    }


def _calculate_trend(values: List[float]) -> str:
    """Calculate trend direction from value series."""
    if len(values) < 3:
        return "stable"

    first_half = sum(values[:len(values)//2]) / (len(values)//2)
    second_half = sum(values[len(values)//2:]) / (len(values) - len(values)//2)

    diff = second_half - first_half
    if diff > 0.05:
        return "improving"
    elif diff < -0.05:
        return "declining"
    return "stable"


def _matches_pattern_signals(text: str, pattern_id: str) -> bool:
    """Check if text matches pattern signals."""
    signals = {
        'debugging': ['error', 'fix', 'bug', 'debug', 'broken'],
        'research': ['arxiv', 'paper', 'research', 'study'],
        'refactoring': ['refactor', 'clean', 'restructure'],
        'testing': ['test', 'spec', 'coverage'],
        'architecture': ['architecture', 'design', 'system'],
        'performance': ['performance', 'optimize', 'slow'],
        'deployment': ['deploy', 'release', 'production'],
        'learning': ['learn', 'understand', 'explain']
    }

    for signal in signals.get(pattern_id, []):
        if signal in text:
            return True
    return False


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN ANALYSIS
# ═══════════════════════════════════════════════════════════════════════════

def analyze_patterns(telemetry: Dict[str, Any]) -> Dict[str, Any]:
    """Analyze patterns and generate insights."""
    insights = []
    recommendations = []

    summary = telemetry.get('summary', {})
    patterns = telemetry.get('patterns', {})
    temporal = telemetry.get('temporal', {})
    identity = telemetry.get('identity', {})

    # Cache efficiency insight
    cache_eff = summary.get('cacheEfficiency', 0)
    if cache_eff > 99.5:
        insights.append({
            "type": "positive",
            "metric": "cacheEfficiency",
            "value": cache_eff,
            "message": f"Excellent cache efficiency at {cache_eff}%. Context reuse is optimal."
        })
    elif cache_eff < 95:
        insights.append({
            "type": "warning",
            "metric": "cacheEfficiency",
            "value": cache_eff,
            "message": f"Cache efficiency at {cache_eff}%. Consider longer sessions or context batching."
        })
        recommendations.append({
            "type": "behavior",
            "action": "Batch related queries into fewer sessions",
            "impact": "Could improve cache efficiency by 2-5%"
        })

    # DQ score insight
    dq_avg = summary.get('dqScoreAvg', 0)
    dq_trend = summary.get('dqTrend', 'stable')
    if dq_avg > 0.85:
        insights.append({
            "type": "positive",
            "metric": "dqScore",
            "value": dq_avg,
            "message": f"High decision quality ({dq_avg}). Model routing is well-calibrated."
        })
    elif dq_avg < 0.7:
        recommendations.append({
            "type": "calibration",
            "action": "Recalibrate DQ weights based on feedback correlation",
            "impact": "Could improve routing accuracy by 10-15%"
        })

    if dq_trend == "declining":
        recommendations.append({
            "type": "attention",
            "action": "Review recent routing decisions for mismatches",
            "impact": "Prevent further DQ score decline"
        })

    # Pattern distribution insight
    distribution = patterns.get('distribution', {})
    if distribution:
        dominant = patterns.get('dominant')
        dominant_pct = distribution.get(dominant, 0) / sum(distribution.values()) * 100 if distribution else 0

        insights.append({
            "type": "pattern",
            "metric": "sessionType",
            "value": dominant,
            "message": f"Dominant pattern: {dominant} ({dominant_pct:.0f}% of activity)"
        })

        if dominant_pct > 50:
            recommendations.append({
                "type": "context",
                "action": f"Pre-load {dominant}-specific context via prefetch --pattern {dominant}",
                "impact": "Faster session starts, more relevant suggestions"
            })

    # Temporal insight
    peak_hours = temporal.get('peakHours', [])
    if peak_hours:
        peak_str = ', '.join(f"{h}:00" for h in peak_hours[:3])
        insights.append({
            "type": "temporal",
            "metric": "peakHours",
            "value": peak_hours,
            "message": f"Peak productivity hours: {peak_str}"
        })

    # Session count insight
    total_sessions = summary.get('totalSessions', 0)
    total_messages = summary.get('totalMessages', 0)
    if total_sessions > 0:
        avg_messages = total_messages / total_sessions
        if avg_messages > 100:
            recommendations.append({
                "type": "efficiency",
                "action": "Consider /clear more often - avg session is {:.0f} messages".format(avg_messages),
                "impact": "Maintain context quality and cache efficiency"
            })

    return {
        "analyzedAt": datetime.now().isoformat(),
        "insights": insights,
        "recommendations": recommendations,
        "confidence": _calculate_confidence(telemetry)
    }


def _calculate_confidence(telemetry: Dict[str, Any]) -> float:
    """Calculate confidence in analysis based on data volume."""
    raw = telemetry.get('rawCounts', {})
    dq_count = raw.get('dqScores', 0)
    activity_count = raw.get('activityEvents', 0)

    # More data = higher confidence (caps at 0.95)
    data_score = min(0.95, (dq_count + activity_count) / 200)

    # Sessions count bonus
    sessions = telemetry.get('summary', {}).get('totalSessions', 0)
    session_score = min(0.3, sessions / 100)

    return round(min(0.95, data_score + session_score), 2)


# ═══════════════════════════════════════════════════════════════════════════
# MODIFICATION GENERATION
# ═══════════════════════════════════════════════════════════════════════════

def generate_modifications(analysis: Dict[str, Any], telemetry: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Generate modification proposals from analysis."""
    modifications = []
    mod_id_base = datetime.now().strftime("%Y%m%d")

    recommendations = analysis.get('recommendations', [])
    insights = analysis.get('insights', [])
    confidence = analysis.get('confidence', 0)

    for i, rec in enumerate(recommendations):
        mod_id = f"mod-{mod_id_base}-{i:03d}"

        mod = {
            "id": mod_id,
            "createdAt": datetime.now().isoformat(),
            "type": rec.get('type'),
            "action": rec.get('action'),
            "impact": rec.get('impact'),
            "confidence": confidence,
            "status": "proposed",
            "target": _determine_target(rec),
            "changes": _generate_changes(rec, telemetry)
        }

        modifications.append(mod)

    # Generate CLAUDE.md update modification
    claude_md_mod = _generate_claude_md_update(insights, telemetry)
    if claude_md_mod:
        modifications.append(claude_md_mod)

    return modifications


def _determine_target(recommendation: Dict[str, Any]) -> str:
    """Determine which file/component the modification targets."""
    rec_type = recommendation.get('type', '')

    if rec_type in ['behavior', 'attention']:
        return 'CLAUDE.md'
    elif rec_type == 'calibration':
        return 'dq-scorer.js'
    elif rec_type == 'context':
        return 'prefetch.py'
    elif rec_type == 'efficiency':
        return 'CLAUDE.md'

    return 'CLAUDE.md'


def _generate_changes(recommendation: Dict[str, Any], telemetry: Dict[str, Any]) -> Dict[str, Any]:
    """Generate specific changes for a recommendation."""
    rec_type = recommendation.get('type', '')

    if rec_type == 'calibration':
        # DQ weight recalibration
        return {
            "file": "~/.claude/kernel/dq-scorer.js",
            "type": "weight_adjustment",
            "current": {"validity": 0.4, "specificity": 0.3, "correctness": 0.3},
            "proposed": {"validity": 0.35, "specificity": 0.35, "correctness": 0.3}
        }

    if rec_type == 'context':
        pattern = telemetry.get('patterns', {}).get('dominant', 'general')
        return {
            "file": "~/.claude/CLAUDE.md",
            "type": "add_instruction",
            "section": "Learned Patterns",
            "content": f"Default prefetch pattern: {pattern}"
        }

    return {"type": "note", "content": recommendation.get('action', '')}


def _generate_claude_md_update(insights: List[Dict], telemetry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Generate CLAUDE.md learned patterns section update."""
    if not insights:
        return None

    summary = telemetry.get('summary', {})
    patterns = telemetry.get('patterns', {})
    temporal = telemetry.get('temporal', {})

    # Build the auto-generated section
    lines = [
        "",
        "## Learned Patterns",
        "",
        LEARNED_START,
        f"<!-- Last Updated: {datetime.now().isoformat()} -->",
        "",
        "### Usage Patterns Observed",
    ]

    # Peak hours
    peak_hours = temporal.get('peakHours', [])
    if peak_hours:
        peak_str = ', '.join(f"{h}:00" for h in peak_hours[:3])
        sessions = summary.get('totalSessions', 0)
        lines.append(f"- Peak productivity hours: {peak_str}")

    # Dominant pattern
    dominant = patterns.get('dominant')
    distribution = patterns.get('distribution', {})
    if dominant and distribution:
        pct = distribution.get(dominant, 0) / sum(distribution.values()) * 100
        lines.append(f"- Dominant session type: {dominant} ({pct:.0f}%)")

    # Session stats
    avg_duration = summary.get('totalMessages', 0) / max(1, summary.get('totalSessions', 1))
    lines.append(f"- Average session length: {avg_duration:.0f} messages")

    lines.append("")
    lines.append("### Optimized Behaviors")

    # Add behavioral recommendations
    if dominant == 'debugging':
        lines.append("- For debugging: Start with /debug, escalate to Opus if >3 iterations")
    elif dominant == 'research':
        lines.append("- For research: Pre-load learnings via prefetch --papers")
    elif dominant == 'refactoring':
        lines.append("- For refactoring: Run tests before AND after changes")
    else:
        lines.append("- Use pattern-aware prefetch for faster context loading")

    # Cache efficiency note
    cache_eff = summary.get('cacheEfficiency', 0)
    lines.append(f"- Cache efficiency: {cache_eff}% - maintain by reusing context")

    # Batch threshold
    lines.append("- Batch threshold: 3 sequential requests -> suggest batching")

    lines.append("")
    lines.append(LEARNED_END)
    lines.append("")

    return {
        "id": f"mod-{datetime.now().strftime('%Y%m%d')}-claude-md",
        "createdAt": datetime.now().isoformat(),
        "type": "claude_md_update",
        "action": "Update CLAUDE.md with learned patterns",
        "impact": "Better session initialization and context awareness",
        "confidence": 0.9,
        "status": "proposed",
        "target": "CLAUDE.md",
        "changes": {
            "file": str(CLAUDE_MD),
            "type": "section_update",
            "section": "Learned Patterns",
            "content": '\n'.join(lines)
        }
    }


# ═══════════════════════════════════════════════════════════════════════════
# MODIFICATION APPLICATION
# ═══════════════════════════════════════════════════════════════════════════

def apply_modification(mod_id: str, dry_run: bool = False) -> Dict[str, Any]:
    """Apply a modification with rollback support."""

    # Load modification
    mod = _find_modification(mod_id)
    if not mod:
        return {"success": False, "error": f"Modification {mod_id} not found"}

    if mod.get('status') == 'applied':
        return {"success": False, "error": f"Modification {mod_id} already applied"}

    changes = mod.get('changes', {})
    target_file = changes.get('file', '')
    change_type = changes.get('type', '')

    result = {"mod_id": mod_id, "dry_run": dry_run}

    if change_type == 'section_update' and 'CLAUDE.md' in target_file:
        # Handle CLAUDE.md section update
        backup_path = _backup_claude_md()
        result['backup'] = str(backup_path)

        if not dry_run:
            success = _update_claude_md_section(changes.get('content', ''))
            if success:
                _log_modification(mod_id, 'applied', backup_path)
                result['success'] = True
                result['message'] = "CLAUDE.md updated with learned patterns"
            else:
                result['success'] = False
                result['error'] = "Failed to update CLAUDE.md"
        else:
            result['success'] = True
            result['message'] = f"Would update CLAUDE.md (backup at {backup_path})"
            result['preview'] = changes.get('content', '')[:500]

    else:
        result['success'] = False
        result['error'] = f"Unknown change type: {change_type}"

    return result


def _find_modification(mod_id: str) -> Optional[Dict[str, Any]]:
    """Find a modification by ID."""
    if not MODIFICATIONS_LOG.exists():
        return None

    for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
        if line:
            try:
                mod = json.loads(line)
                if mod.get('id') == mod_id:
                    return mod
            except:
                pass

    return None


def _backup_claude_md() -> Path:
    """Backup CLAUDE.md before modification."""
    HISTORY_DIR.mkdir(parents=True, exist_ok=True)

    if CLAUDE_MD.exists():
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = HISTORY_DIR / f"CLAUDE.md.{timestamp}"
        backup_path.write_text(CLAUDE_MD.read_text())

        # Initialize git if not exists
        git_dir = HISTORY_DIR / ".git"
        if not git_dir.exists():
            subprocess.run(['git', 'init'], cwd=HISTORY_DIR, capture_output=True)

        # Commit the backup
        subprocess.run(['git', 'add', '.'], cwd=HISTORY_DIR, capture_output=True)
        subprocess.run(['git', 'commit', '-m', f'Backup before {timestamp}'],
                      cwd=HISTORY_DIR, capture_output=True)

        return backup_path

    return HISTORY_DIR / "CLAUDE.md.empty"


def _update_claude_md_section(new_content: str) -> bool:
    """Update the learned patterns section in CLAUDE.md."""
    if not CLAUDE_MD.exists():
        return False

    content = CLAUDE_MD.read_text()

    # Check if section exists
    if LEARNED_START in content and LEARNED_END in content:
        # Replace existing section
        pattern = rf"## Learned Patterns\s*\n{re.escape(LEARNED_START)}.*?{re.escape(LEARNED_END)}"
        new_content_clean = new_content.strip()
        content = re.sub(pattern, new_content_clean, content, flags=re.DOTALL)
    else:
        # Add new section at end
        content = content.rstrip() + "\n\n" + new_content.strip() + "\n"

    CLAUDE_MD.write_text(content)
    return True


def _log_modification(mod_id: str, status: str, backup_path: Optional[Path] = None):
    """Log modification application."""
    entry = {
        "mod_id": mod_id,
        "status": status,
        "appliedAt": datetime.now().isoformat(),
        "backup": str(backup_path) if backup_path else None
    }

    with open(MODIFICATIONS_LOG, 'a') as f:
        f.write(json.dumps(entry) + '\n')


def rollback_modification(mod_id: str) -> Dict[str, Any]:
    """Rollback a modification."""
    # Find the modification and its backup
    if not MODIFICATIONS_LOG.exists():
        return {"success": False, "error": "No modifications log found"}

    for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
        if line:
            try:
                entry = json.loads(line)
                if entry.get('mod_id') == mod_id and entry.get('backup'):
                    backup_path = Path(entry['backup'])
                    if backup_path.exists():
                        CLAUDE_MD.write_text(backup_path.read_text())
                        _log_modification(mod_id, 'rolled_back')
                        return {
                            "success": True,
                            "message": f"Rolled back {mod_id} from {backup_path}"
                        }
            except:
                pass

    return {"success": False, "error": f"No backup found for {mod_id}"}


# ═══════════════════════════════════════════════════════════════════════════
# EFFECTIVENESS TRACKING
# ═══════════════════════════════════════════════════════════════════════════

def evaluate_effectiveness(mod_id: str, sessions: int = 10) -> Dict[str, Any]:
    """Evaluate effectiveness of a modification."""
    # Load before/after metrics
    stats = load_stats_cache()
    dq_scores = load_dq_scores()

    # Find modification timestamp
    mod = _find_modification(mod_id)
    if not mod:
        return {"success": False, "error": f"Modification {mod_id} not found"}

    applied_at = mod.get('appliedAt')
    if not applied_at:
        return {"success": False, "error": "Modification not applied"}

    applied_ts = datetime.fromisoformat(applied_at).timestamp() * 1000

    # Split scores into before/after
    before = [s for s in dq_scores if s.get('ts', 0) < applied_ts]
    after = [s for s in dq_scores if s.get('ts', 0) >= applied_ts]

    if len(after) < sessions:
        return {
            "success": False,
            "error": f"Not enough sessions yet ({len(after)}/{sessions})"
        }

    # Calculate metrics
    before_dq = sum(s.get('dqScore', 0) for s in before[-sessions:]) / sessions if before else 0
    after_dq = sum(s.get('dqScore', 0) for s in after[:sessions]) / sessions

    improvement = after_dq - before_dq
    significant = abs(improvement) > 0.05

    result = {
        "mod_id": mod_id,
        "metric": "dq_score_avg",
        "before": round(before_dq, 3),
        "after": round(after_dq, 3),
        "improvement": round(improvement, 3),
        "sessionsCompared": sessions,
        "statisticallySignificant": significant,
        "evaluatedAt": datetime.now().isoformat()
    }

    # Log effectiveness
    with open(EFFECTIVENESS_LOG, 'a') as f:
        f.write(json.dumps(result) + '\n')

    return result


def get_dashboard() -> Dict[str, Any]:
    """Get dashboard view of co-evolution system."""
    config = load_config()
    telemetry = aggregate_telemetry(days=config['trackingWindow']['analysis'])

    # Load modification history
    modifications = []
    if MODIFICATIONS_LOG.exists():
        for line in MODIFICATIONS_LOG.read_text().strip().split('\n'):
            if line:
                try:
                    modifications.append(json.loads(line))
                except:
                    pass

    # Load effectiveness history
    effectiveness = []
    if EFFECTIVENESS_LOG.exists():
        for line in EFFECTIVENESS_LOG.read_text().strip().split('\n'):
            if line:
                try:
                    effectiveness.append(json.loads(line))
                except:
                    pass

    return {
        "generatedAt": datetime.now().isoformat(),
        "config": {
            "enabled": config.get('enabled', True),
            "autoApply": config.get('autoApply', False),
            "minConfidence": config.get('minConfidence', 0.7)
        },
        "summary": telemetry.get('summary', {}),
        "patterns": telemetry.get('patterns', {}),
        "modifications": {
            "total": len(modifications),
            "applied": len([m for m in modifications if m.get('status') == 'applied']),
            "rolledBack": len([m for m in modifications if m.get('status') == 'rolled_back']),
            "recent": modifications[-5:]
        },
        "effectiveness": {
            "evaluations": len(effectiveness),
            "avgImprovement": sum(e.get('improvement', 0) for e in effectiveness) / len(effectiveness) if effectiveness else 0,
            "recent": effectiveness[-5:]
        }
    }


# ═══════════════════════════════════════════════════════════════════════════
# ROUTING OPTIMIZER (Phase 4)
# ═══════════════════════════════════════════════════════════════════════════

class RoutingOptimizer:
    """Domain-specific optimizer for CLI routing system"""

    def __init__(self):
        self.metrics_file = Path.home() / ".claude/data/routing-metrics.jsonl"
        self.history_file = Path.home() / ".claude/kernel/dq-scores.jsonl"
        self.baselines_file = Path.home() / ".claude/kernel/baselines.json"

    def analyze_routing_patterns(self, days: int = 30) -> Dict[str, Any]:
        """Analyze routing patterns and identify optimization opportunities"""
        metrics = self._load_metrics(days)
        history = self._load_history()

        if not metrics and not history:
            return {"error": "No routing data available"}

        opportunities = []

        # Pattern 1: Consistent over-provisioning
        overprovisioned = self._find_overprovisioned(metrics, history)
        if overprovisioned:
            opportunities.append({
                "type": "threshold_adjustment",
                "issue": "Frequent over-provisioning detected",
                "details": overprovisioned,
                "proposal": "Increase threshold for cheaper model"
            })

        # Pattern 2: High complexity queries failing
        failures = self._find_complexity_failures(history)
        if failures:
            opportunities.append({
                "type": "capability_gap",
                "issue": "High complexity queries underperforming",
                "details": failures,
                "proposal": "Route more aggressively to opus"
            })

        # Pattern 3: Routing accuracy below target
        accuracy = self._calculate_accuracy(history)
        if accuracy and accuracy < 0.75:
            opportunities.append({
                "type": "accuracy_improvement",
                "issue": f"Routing accuracy ({accuracy:.1%}) below 75% target",
                "details": {"current_accuracy": accuracy, "target": 0.75},
                "proposal": "Adjust DQ weights or complexity thresholds"
            })

        # Pattern 4: Cost efficiency opportunities
        cost_analysis = self._analyze_cost_efficiency(metrics)
        if cost_analysis.get('potential_savings', 0) > 0.10:
            opportunities.append({
                "type": "cost_optimization",
                "issue": f"Potential cost savings: {cost_analysis['potential_savings']:.1%}",
                "details": cost_analysis,
                "proposal": "Optimize model selection thresholds"
            })

        return {
            "period_days": days,
            "total_queries": len(metrics),
            "accuracy": accuracy,
            "opportunities": opportunities,
            "confidence": self._calculate_confidence(opportunities, len(metrics))
        }

    def generate_routing_proposals(self, analysis: Dict) -> List[Dict]:
        """Generate actionable proposals from routing analysis"""
        if 'error' in analysis:
            return []

        baselines = self._load_baselines()
        if not baselines:
            return []

        proposals = []
        opportunities = analysis.get('opportunities', [])

        for opp in opportunities:
            if opp['type'] == 'threshold_adjustment':
                # Generate threshold adjustment proposals
                for model, data in opp['details'].items():
                    current = baselines['complexity_thresholds'][model]['range'][1]
                    proposed = min(1.0, current + 0.02)  # Incremental +2% adjustment

                    proposals.append({
                        "id": f"routing-{len(proposals)+1:03d}",
                        "type": "threshold_adjustment",
                        "domain": "routing",
                        "target": f"complexity_thresholds.{model}.range[1]",
                        "current": current,
                        "proposed": proposed,
                        "rationale": f"Observed {data['overprovisioning_rate']:.1%} over-provisioning for {model} ({data['samples']} samples)",
                        "expected_impact": "Cost reduction, maintained quality",
                        "confidence": min(0.9, 0.6 + (data['samples'] / 100) * 0.3),
                        "ab_test_recommended": True,
                        "created": datetime.now().isoformat()
                    })

            elif opp['type'] == 'capability_gap':
                # Adjust sonnet/opus boundary
                current = baselines['complexity_thresholds']['sonnet']['range'][1]
                proposed = max(0.5, current - 0.02)  # Lower threshold to route to opus earlier

                proposals.append({
                    "id": f"routing-{len(proposals)+1:03d}",
                    "type": "capability_gap_fix",
                    "domain": "routing",
                    "target": "complexity_thresholds.sonnet.range[1]",
                    "current": current,
                    "proposed": proposed,
                    "rationale": f"High complexity queries underperforming (failure rate: {opp['details'].get('failure_rate', 0):.1%})",
                    "expected_impact": "Improved accuracy for complex queries",
                    "confidence": 0.75,
                    "ab_test_recommended": True,
                    "created": datetime.now().isoformat()
                })

            elif opp['type'] == 'accuracy_improvement':
                # Propose DQ weight adjustments
                proposals.append({
                    "id": f"routing-{len(proposals)+1:03d}",
                    "type": "dq_weight_tuning",
                    "domain": "routing",
                    "target": "dq_weights",
                    "current": baselines.get('dq_weights', {}),
                    "proposed": {
                        "validity": 0.35,
                        "specificity": 0.35,
                        "correctness": 0.30
                    },
                    "rationale": f"Routing accuracy ({opp['details']['current_accuracy']:.1%}) below target",
                    "expected_impact": "Improved routing accuracy",
                    "confidence": 0.70,
                    "ab_test_recommended": True,
                    "created": datetime.now().isoformat()
                })

        return proposals

    def _load_metrics(self, days: int) -> List[Dict]:
        """Load routing metrics"""
        if not self.metrics_file.exists():
            return []

        cutoff = datetime.now() - timedelta(days=days)
        metrics = []

        with open(self.metrics_file) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    entry = json.loads(line)
                    entry_time = datetime.fromtimestamp(entry['ts'])
                    if entry_time > cutoff:
                        metrics.append(entry)
                except (json.JSONDecodeError, KeyError):
                    continue

        return metrics

    def _load_history(self) -> List[Dict]:
        """Load DQ scoring history"""
        if not self.history_file.exists():
            return []

        history = []
        with open(self.history_file) as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    history.append(json.loads(line))
                except json.JSONDecodeError:
                    continue

        return history

    def _load_baselines(self) -> Optional[Dict]:
        """Load baseline configuration"""
        if not self.baselines_file.exists():
            return None

        try:
            return json.loads(self.baselines_file.read_text())
        except (json.JSONDecodeError, IOError):
            return None

    def _find_overprovisioned(self, metrics: List[Dict], history: List[Dict]) -> Dict:
        """Identify models being over-provisioned"""
        from collections import defaultdict

        # Find cases where cheaper model would have sufficed
        overprovisioning = defaultdict(lambda: {"count": 0, "total": 0})

        for entry in history:
            if 'success' not in entry:
                continue

            model = entry.get('model', 'sonnet')
            complexity = entry.get('complexity', {}).get('score', 0.5)
            success = entry.get('success', False)

            overprovisioning[model]['total'] += 1

            # Check if cheaper model could handle this
            if model == 'opus' and complexity < 0.65 and success:
                overprovisioning['opus']['count'] += 1
            elif model == 'sonnet' and complexity < 0.28 and success:
                overprovisioning['sonnet']['count'] += 1

        # Filter significant over-provisioning (>15%)
        result = {}
        for model, data in overprovisioning.items():
            if data['total'] > 20:  # Minimum sample size
                rate = data['count'] / data['total']
                if rate > 0.15:
                    result[model] = {
                        "overprovisioning_rate": rate,
                        "samples": data['total'],
                        "overprovisioned_count": data['count']
                    }

        return result

    def _find_complexity_failures(self, history: List[Dict]) -> Optional[Dict]:
        """Find high complexity queries that failed"""
        high_complexity_failures = []

        for entry in history:
            if 'success' not in entry:
                continue

            complexity = entry.get('complexity', {}).get('score', 0.5)
            success = entry.get('success', False)
            model = entry.get('model', 'sonnet')

            if complexity > 0.60 and not success:
                high_complexity_failures.append({
                    "complexity": complexity,
                    "model": model
                })

        if len(high_complexity_failures) > 5:  # Significant pattern
            failure_rate = len(high_complexity_failures) / len([h for h in history if h.get('complexity', {}).get('score', 0) > 0.60])
            return {
                "failure_rate": failure_rate,
                "failures": len(high_complexity_failures),
                "complexity_range": [0.60, 1.0]
            }

        return None

    def _calculate_accuracy(self, history: List[Dict]) -> Optional[float]:
        """Calculate routing accuracy from feedback"""
        with_feedback = [h for h in history if 'success' in h]

        if len(with_feedback) < 10:  # Minimum sample size
            return None

        successful = [h for h in with_feedback if h['success']]
        return len(successful) / len(with_feedback)

    def _analyze_cost_efficiency(self, metrics: List[Dict]) -> Dict:
        """Analyze cost efficiency opportunities"""
        if not metrics:
            return {"potential_savings": 0}

        # Simple heuristic: check if we're using more expensive models than necessary
        model_counts = {"haiku": 0, "sonnet": 0, "opus": 0}

        for m in metrics:
            model_counts[m.get('model', 'sonnet')] += 1

        total = sum(model_counts.values())
        if total == 0:
            return {"potential_savings": 0}

        # Ideal distribution: 35% haiku, 50% sonnet, 15% opus
        # Current distribution vs ideal gives potential savings
        current_opus_rate = model_counts['opus'] / total
        current_sonnet_rate = model_counts['sonnet'] / total

        potential_savings = 0
        if current_opus_rate > 0.20:  # Using opus too much
            potential_savings += (current_opus_rate - 0.15) * 0.50

        if current_sonnet_rate > 0.55:  # Using sonnet too much
            potential_savings += (current_sonnet_rate - 0.50) * 0.20

        return {
            "potential_savings": min(0.30, potential_savings),
            "current_distribution": model_counts,
            "ideal_distribution": {"haiku": 0.35, "sonnet": 0.50, "opus": 0.15}
        }

    def _calculate_confidence(self, opportunities: List[Dict], sample_size: int) -> float:
        """Calculate confidence score for analysis"""
        if not opportunities:
            return 0.0

        if sample_size < 50:
            return 0.5  # Low confidence with small sample

        if sample_size < 100:
            return 0.7  # Medium confidence

        return 0.85  # High confidence with large sample


# ═══════════════════════════════════════════════════════════════════════════
# CLI INTERFACE
# ═══════════════════════════════════════════════════════════════════════════

def main():
    parser = argparse.ArgumentParser(
        description="Meta-Analyzer: Bidirectional Co-Evolution System"
    )

    subparsers = parser.add_subparsers(dest='command', help='Commands')

    # analyze
    analyze_parser = subparsers.add_parser('analyze', help='Run analysis and show insights')
    analyze_parser.add_argument('--days', '-d', type=int, default=7, help='Days to analyze')
    analyze_parser.add_argument('--json', action='store_true', help='Output as JSON')

    # propose
    propose_parser = subparsers.add_parser('propose', help='Generate modification proposals')
    propose_parser.add_argument('--json', action='store_true', help='Output as JSON')
    propose_parser.add_argument('--domain', choices=['general', 'routing'], default='general', help='Domain-specific analysis')
    propose_parser.add_argument('--days', type=int, default=30, help='Days to analyze (routing only)')

    # apply
    apply_parser = subparsers.add_parser('apply', help='Apply a modification')
    apply_parser.add_argument('mod_id', help='Modification ID')
    apply_parser.add_argument('--dry-run', action='store_true', help='Preview without applying')

    # rollback
    rollback_parser = subparsers.add_parser('rollback', help='Rollback a modification')
    rollback_parser.add_argument('mod_id', help='Modification ID')

    # evaluate
    eval_parser = subparsers.add_parser('evaluate', help='Evaluate modification effectiveness')
    eval_parser.add_argument('mod_id', help='Modification ID')
    eval_parser.add_argument('--sessions', type=int, default=10, help='Sessions to compare')

    # dashboard
    dash_parser = subparsers.add_parser('dashboard', help='View system dashboard')
    dash_parser.add_argument('--json', action='store_true', help='Output as JSON')

    # config
    config_parser = subparsers.add_parser('config', help='View/modify configuration')
    config_parser.add_argument('--set', nargs=2, metavar=('KEY', 'VALUE'), help='Set config value')

    args = parser.parse_args()

    if args.command == 'analyze':
        telemetry = aggregate_telemetry(days=args.days)
        analysis = analyze_patterns(telemetry)

        if args.json:
            print(json.dumps({"telemetry": telemetry, "analysis": analysis}, indent=2))
        else:
            print(f"\n{'='*60}")
            print("META-ANALYZER: Co-Evolution Analysis")
            print(f"{'='*60}\n")

            summary = telemetry.get('summary', {})
            print(f"Sessions: {summary.get('totalSessions', 0)} | Messages: {summary.get('totalMessages', 0)}")
            print(f"Cache Efficiency: {summary.get('cacheEfficiency', 0)}%")
            print(f"DQ Score Avg: {summary.get('dqScoreAvg', 0)} ({summary.get('dqTrend', 'stable')})")
            print(f"Confidence: {analysis.get('confidence', 0)}")

            print(f"\n--- Insights ---")
            for insight in analysis.get('insights', []):
                icon = "+" if insight['type'] == 'positive' else "!" if insight['type'] == 'warning' else "*"
                print(f"  [{icon}] {insight['message']}")

            print(f"\n--- Recommendations ---")
            for rec in analysis.get('recommendations', []):
                print(f"  -> {rec['action']}")
                print(f"     Impact: {rec['impact']}")

            print()

    elif args.command == 'propose':
        # Domain-specific analysis
        if args.domain == 'routing':
            optimizer = RoutingOptimizer()
            analysis = optimizer.analyze_routing_patterns(days=args.days)

            if 'error' in analysis:
                print(f"❌ {analysis['error']}")
                return

            modifications = optimizer.generate_routing_proposals(analysis)

            if args.json:
                print(json.dumps({"analysis": analysis, "proposals": modifications}, indent=2))
            else:
                print(f"\n{'='*70}")
                print("ROUTING OPTIMIZER: Pattern Analysis")
                print(f"{'='*70}\n")

                print(f"Period: {analysis['period_days']} days")
                print(f"Total Queries: {analysis['total_queries']}")
                if analysis.get('accuracy'):
                    print(f"Routing Accuracy: {analysis['accuracy']:.1%}")
                print(f"Analysis Confidence: {analysis['confidence']:.2f}")

                print(f"\n--- Optimization Opportunities ---")
                for opp in analysis.get('opportunities', []):
                    print(f"\n  Type: {opp['type']}")
                    print(f"  Issue: {opp['issue']}")
                    print(f"  Proposal: {opp['proposal']}")

                print(f"\n--- Generated Proposals ({len(modifications)}) ---")
                for mod in modifications:
                    print(f"\n  {mod['id']}: {mod['type']}")
                    print(f"     Target: {mod['target']}")
                    print(f"     Current: {mod['current']} → Proposed: {mod['proposed']}")
                    print(f"     Rationale: {mod['rationale']}")
                    print(f"     Confidence: {mod['confidence']:.2f}")
                    if mod.get('ab_test_recommended'):
                        print(f"     ⚠️  A/B test recommended before applying")

                if modifications:
                    print(f"\nNext steps:")
                    print(f"  1. Review proposals above")
                    print(f"  2. Create A/B test: routing-metrics.py ab-test create --config <config.json>")
                    print(f"  3. Or apply directly: meta-analyzer apply <mod_id> --dry-run")

        else:
            # General analysis
            telemetry = aggregate_telemetry()
            analysis = analyze_patterns(telemetry)
            modifications = generate_modifications(analysis, telemetry)

            # Save proposals
            for mod in modifications:
                with open(MODIFICATIONS_LOG, 'a') as f:
                    f.write(json.dumps(mod) + '\n')

            if args.json:
                print(json.dumps(modifications, indent=2))
            else:
                print(f"\nGenerated {len(modifications)} modification proposals:\n")
                for mod in modifications:
                    print(f"  {mod['id']}: {mod['action']}")
                    print(f"     Type: {mod['type']} | Target: {mod['target']}")
                    print(f"     Confidence: {mod['confidence']} | Impact: {mod.get('impact', 'N/A')}")
                    print()

                print("Apply with: meta-analyzer apply <mod_id>")
                print("Preview with: meta-analyzer apply <mod_id> --dry-run")

    elif args.command == 'apply':
        result = apply_modification(args.mod_id, dry_run=args.dry_run)
        print(json.dumps(result, indent=2))

    elif args.command == 'rollback':
        result = rollback_modification(args.mod_id)
        print(json.dumps(result, indent=2))

    elif args.command == 'evaluate':
        result = evaluate_effectiveness(args.mod_id, sessions=args.sessions)
        print(json.dumps(result, indent=2))

    elif args.command == 'dashboard':
        dashboard = get_dashboard()

        if args.json:
            print(json.dumps(dashboard, indent=2))
        else:
            print(f"\n{'='*60}")
            print("CO-EVOLUTION DASHBOARD")
            print(f"{'='*60}\n")

            config = dashboard.get('config', {})
            print(f"Status: {'ENABLED' if config.get('enabled') else 'DISABLED'}")
            print(f"Auto-Apply: {'ON' if config.get('autoApply') else 'OFF'}")
            print(f"Min Confidence: {config.get('minConfidence', 0.7)}")

            summary = dashboard.get('summary', {})
            print(f"\n--- Metrics ---")
            print(f"Cache Efficiency: {summary.get('cacheEfficiency', 0)}%")
            print(f"DQ Score Avg: {summary.get('dqScoreAvg', 0)}")

            mods = dashboard.get('modifications', {})
            print(f"\n--- Modifications ---")
            print(f"Total: {mods.get('total', 0)} | Applied: {mods.get('applied', 0)} | Rolled Back: {mods.get('rolledBack', 0)}")

            eff = dashboard.get('effectiveness', {})
            print(f"\n--- Effectiveness ---")
            print(f"Evaluations: {eff.get('evaluations', 0)}")
            print(f"Avg Improvement: {eff.get('avgImprovement', 0):.3f}")
            print()

    elif args.command == 'config':
        config = load_config()

        if args.set:
            key, value = args.set
            # Parse value
            if value.lower() == 'true':
                value = True
            elif value.lower() == 'false':
                value = False
            elif value.replace('.', '').isdigit():
                value = float(value) if '.' in value else int(value)

            config[key] = value
            save_config(config)
            print(f"Set {key} = {value}")
        else:
            print(json.dumps(config, indent=2))

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
