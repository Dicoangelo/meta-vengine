# SQLite Migration Plan - Fixing the JSONL Mistake

**Created**: 2026-01-28
**Status**: üî¥ CRITICAL - Technical debt causing data duplication and sync overhead
**Priority**: HIGH

## The Problem

### Current (Broken) Architecture

```
Hooks ‚Üí SQLite ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îú‚Üí Sync Scripts ‚Üí JSONL ‚Üí Dashboard reads JSONL
Backfill ‚Üí JSONL ‚îÄ‚îÄ‚îò
```

**Issues:**
1. ‚úÖ SQLite is hooked up and receiving data in real-time
2. ‚ùå Dashboard (ccc-generator.sh) reads JSONL files, NOT SQLite
3. ‚ùå Sync scripts duplicate SQLite ‚Üí JSONL
4. ‚ùå Backfill writes to JSONL, then must sync to SQLite
5. ‚ùå Data exists in TWO places (SQLite + JSONL)
6. ‚ùå JSONL files are huge (9.9MB activity-events, 4.9MB tool-usage)

### Why This Happened

**Timeline:**
1. **Early 2025**: Command Center built, reads from simple JSONL files
2. **Mid 2025**: SQLite databases added for structured storage
3. **Late 2025**: Sync scripts created to bridge SQLite ‚Üî JSONL
4. **Jan 2026**: Dashboard still reads JSONL, never migrated to SQLite

**Root cause**: Dashboard was never refactored to read from SQLite.

## Why SQLite is Superior

| Feature | SQLite | JSONL |
|---------|--------|-------|
| Speed | O(log n) indexed queries | O(n) sequential scan |
| Concurrency | Multiple readers | File locking issues |
| Atomicity | ACID transactions | Line-by-line append |
| Queries | SQL (aggregate, join, filter) | Manual parsing |
| Size | Compressed, efficient | Plain text, verbose |
| Corruption | Resilient with WAL mode | One bad line breaks it |

### Real Numbers

```
SQLite databases:
  antigravity.db: 12MB (31 tables, 11,175 tool events)
  supermemory.db: 134MB (11 tables, full knowledge graph)
  claude.db: 404KB (12 tables, aggregated stats)

JSONL files (duplicating SQLite):
  tool-usage.jsonl: 4.9MB (59,880 lines)
  activity-events.jsonl: 9.9MB (69,890 lines)

Wasted space: ~15MB in duplicate JSONL files
Sync overhead: 49 scripts reading JSONL instead of 11 reading SQLite
```

## The Mistake: Why JSONL Was Used

### Valid Initial Reasons

1. **Simplicity**: JSONL is simple, no schema needed
2. **Portability**: Works everywhere, no dependencies
3. **Append-only**: Easy to stream, no transactions needed
4. **Human readable**: Can `cat` and `grep` files

### But These Don't Apply Here

1. ‚ùå **Simplicity**: We now have complex sync scripts (more complex than SQL!)
2. ‚ùå **Portability**: We're already using SQLite everywhere
3. ‚ùå **Append-only**: SQLite with WAL mode is also append-only
4. ‚ùå **Human readable**: Dashboard is the only consumer, not humans

## Migration Plan

### Phase 1: Audit (DONE TODAY)

‚úÖ Identified all JSONL files
‚úÖ Mapped SQLite tables
‚úÖ Found 49 scripts reading JSONL
‚úÖ Documented the problem

### Phase 2: Dashboard Migration (HIGH PRIORITY)

**Goal**: Make `ccc-generator.sh` read from SQLite instead of JSONL

**Changes needed:**

1. **Replace JSONL readers with SQLite queries**
   ```python
   # OLD (JSONL)
   with open('tool-usage.jsonl') as f:
       for line in f:
           data.append(json.loads(line))

   # NEW (SQLite)
   cursor.execute("SELECT * FROM tool_events ORDER BY ts DESC LIMIT 1000")
   data = cursor.fetchall()
   ```

2. **Consolidate data sources**
   - `stats-cache.json` ‚Üí `claude.db` (daily_stats, hourly_activity tables)
   - `tool-usage.jsonl` ‚Üí `antigravity.db` (tool_events table)
   - `session-outcomes.jsonl` ‚Üí `claude.db` (sessions table)
   - `activity-events.jsonl` ‚Üí `antigravity.db` (tool_events table)

3. **Create dashboard data loader**
   ```python
   # ~/.claude/scripts/dashboard-data-loader.py
   # Single script that reads ALL data from SQLite
   # Outputs JSON for ccc-generator.sh to embed
   ```

### Phase 3: Deprecate Sync Scripts (MEDIUM PRIORITY)

**Remove these scripts (no longer needed):**
- `sqlite-to-jsonl-sync.py` ‚Üê Entire purpose is to duplicate data
- `integrate-untracked-data.py` ‚Üê Can write directly to SQLite

**Keep only:**
- `sqlite-hook.py` ‚Üê Real-time hook (writes to SQLite)
- `backfill-*.py` ‚Üê Writes directly to SQLite, no JSONL middleman

### Phase 4: Archive JSONL Files (LOW PRIORITY)

```bash
# Move to archive
mkdir ~/.claude/data/jsonl-archive
mv ~/.claude/data/*.jsonl ~/.claude/data/jsonl-archive/

# Keep only for historical analysis
# Delete after 30 days if no issues
```

## Preventing This Mistake

### Architectural Principles

**1. Single Source of Truth (SSOT)**

```yaml
RULE: Each data point should exist in EXACTLY ONE place
VIOLATION: tool_events exist in both SQLite AND JSONL
FIX: Choose SQLite, delete JSONL
```

**2. Write Once, Read Many**

```yaml
RULE: Don't create sync scripts unless absolutely necessary
VIOLATION: sqlite-to-jsonl-sync.py duplicates data for no reason
FIX: Make consumers read from the source (SQLite)
```

**3. Use the Right Tool**

```yaml
RULE: Structured data ‚Üí Database, Unstructured data ‚Üí Files
VIOLATION: Using JSONL for structured time-series data
FIX: Use SQLite for events, PostgreSQL for bigger deployments
```

**4. Database First**

```yaml
RULE: If you need a database, use it from day one
VIOLATION: Started with JSONL, "added SQLite later"
FIX: SQLite should be the default for ANY persistent data
```

### Decision Checklist

Before using JSONL files, ask:

- [ ] Is this data structured? (columns/fields) ‚Üí Use SQLite
- [ ] Will I need to query/filter? ‚Üí Use SQLite
- [ ] Is this data growing over time? ‚Üí Use SQLite
- [ ] Do I need aggregations (sum, count, avg)? ‚Üí Use SQLite
- [ ] Will multiple processes access this? ‚Üí Use SQLite
- [ ] Is this data important (not just logs)? ‚Üí Use SQLite

**When to use JSONL:**
- ‚úÖ One-time exports for external tools
- ‚úÖ Append-only logs that get rotated
- ‚úÖ Data that will be processed by external tools (not you)

**When to use SQLite:**
- ‚úÖ Application data storage (events, users, sessions)
- ‚úÖ Time-series data
- ‚úÖ Anything you'll query later
- ‚úÖ Structured data with relationships

### Code Review Patterns

**Add to code review checklist:**

```markdown
## Data Storage Review

- [ ] Does this create a new .jsonl file?
  - [ ] Why not SQLite?
  - [ ] Will this need a sync script later?

- [ ] Does this create a sync script?
  - [ ] Can consumers read from source instead?
  - [ ] Is this creating duplicate data?

- [ ] Does this read from JSONL?
  - [ ] Can it read from SQLite instead?
  - [ ] Is SQLite available with this data?
```

## Migration Effort Estimate

### Phase 2: Dashboard Migration
**Time**: 4-6 hours
**Risk**: Medium (dashboard might break temporarily)
**Files**: 5-10 scripts

**Approach:**
1. Create `dashboard-data-loader.py` (reads from SQLite)
2. Test it produces same JSON as current JSONL readers
3. Update `ccc-generator.sh` to use new loader
4. Test dashboard thoroughly
5. Remove old JSONL readers

### Phase 3: Deprecate Sync Scripts
**Time**: 2-3 hours
**Risk**: Low (just deletion)
**Files**: 3-5 scripts

### Phase 4: Archive JSONL
**Time**: 30 minutes
**Risk**: Very low

**Total**: ~8 hours to fully migrate

## Implementation Order

### Immediate (Today)
1. ‚úÖ Document the problem (this file)
2. ‚úÖ Create architectural principles
3. ‚¨ú Add to CLAUDE.md for future reference

### This Week
1. ‚¨ú Create `dashboard-data-loader.py` (SQLite ‚Üí JSON)
2. ‚¨ú Test data loader produces correct output
3. ‚¨ú Update `ccc-generator.sh` to use loader
4. ‚¨ú Test dashboard with SQLite backend

### Next Week
1. ‚¨ú Remove sync scripts
2. ‚¨ú Update hooks to write ONLY to SQLite
3. ‚¨ú Archive JSONL files
4. ‚¨ú Update documentation

### One Month Later
1. ‚¨ú Verify no issues
2. ‚¨ú Delete archived JSONL files
3. ‚¨ú Remove JSONL-related code
4. ‚¨ú Celebrate üéâ

## Success Metrics

| Metric | Before | Target | Benefit |
|--------|--------|--------|---------|
| Data duplication | 15MB | 0MB | -100% |
| Sync scripts | 3 | 0 | Simpler |
| Dashboard load time | ~2s | <0.5s | 4x faster |
| Scripts reading data | 49 | 11 | 78% reduction |
| Data sources | 2 (SQLite+JSONL) | 1 (SQLite) | SSOT |

## Monitoring

After migration:

```bash
# Verify no JSONL files are being written
find ~/.claude/data -name "*.jsonl" -mtime -1

# Verify SQLite is being read
lsof | grep "antigravity.db\|claude.db"

# Dashboard load time
time ccc --no-open

# No sync scripts running
ps aux | grep sync
```

## Lessons Learned

### What Went Wrong

1. **Started simple, never refactored**: JSONL was "good enough" initially
2. **SQLite added, never migrated**: Added SQLite without removing JSONL
3. **Sync scripts = band-aid**: Created bridges instead of fixing root cause
4. **No architectural review**: No one questioned "why both?"

### What to Do Differently

1. **Database first**: Use SQLite from day one for structured data
2. **Migrate, don't bridge**: When adding a database, migrate old code
3. **Question duplication**: If data exists twice, something is wrong
4. **Regular arch reviews**: Monthly check for technical debt

### Quote to Remember

> "If you need a database, use a database. Don't build a worse database out of text files."
> ‚Äî Every experienced developer

## References

- SQLite docs: https://sqlite.org/whentouse.html
- SQLite performance: https://www.sqlite.org/speed.html
- JSONL vs SQLite: https://stackoverflow.com/questions/tagged/sqlite+jsonl

---

## Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 2026-01-28 | Migrate dashboard to SQLite | Eliminate data duplication, improve performance |
| 2026-01-28 | Deprecate JSONL for structured data | SQLite is superior for all our use cases |
| 2026-01-28 | Add architectural principles | Prevent repeating this mistake |

---

**Status**: üìã Plan approved, awaiting implementation
**Owner**: System architect
**Timeline**: 2 weeks
**Priority**: HIGH - Technical debt is creating maintenance burden
